{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a342dd2b-3372-4414-83e0-f0360d7d485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fec818f3-61b7-4b4d-bcef-125bdc6aa49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "ignore_cols = [\"id\", \"date\", \"zipcode\"]\n",
    "for col in ignore_cols:\n",
    "    if col in train.columns:\n",
    "        train = train.drop(columns=col)\n",
    "    if col in test.columns:\n",
    "        test = test.drop(columns=col)\n",
    "\n",
    "train[\"price\"] = train[\"price\"] / 1000\n",
    "test[\"price\"] = test[\"price\"] / 1000\n",
    "\n",
    "X_train = train.drop(\"price\", axis=1)\n",
    "y_train = train[\"price\"]\n",
    "X_test = test.drop(\"price\", axis=1)\n",
    "y_test = test[\"price\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d2860a-0a97-4b88-accd-ff0a8075491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 31415.747916100863\n",
      "Train R2: 0.7271450489303788\n",
      "Test MSE: 58834.673978213985\n",
      "Test R2: 0.6471195893437872\n",
      "          Feature  Coefficient\n",
      "9           grade    92.511076\n",
      "14            lat    78.129852\n",
      "6      waterfront    64.230911\n",
      "3     sqft_living    57.161582\n",
      "10     sqft_above    48.439051\n",
      "7            view    47.610288\n",
      "16  sqft_living15    45.479128\n",
      "11  sqft_basement    27.688812\n",
      "2       bathrooms    18.456913\n",
      "13   yr_renovated    17.341926\n",
      "8       condition    12.647609\n",
      "4        sqft_lot    11.127338\n",
      "0      Unnamed: 0     8.456024\n",
      "5          floors     8.151038\n",
      "15           long    -1.437669\n",
      "1        bedrooms   -12.807339\n",
      "17     sqft_lot15   -12.906560\n",
      "12       yr_built   -68.043173\n"
     ]
    }
   ],
   "source": [
    "######## Problem 2:  Linear regression #######\n",
    "\n",
    "# 1. and 2.\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = model.predict(X_train_scaled)\n",
    "y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Metrics\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Train MSE:\", train_mse)\n",
    "print(\"Train R2:\", train_r2)\n",
    "print(\"Test MSE:\", test_mse)\n",
    "print(\"Test R2:\", test_r2)\n",
    " \n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": X_train.columns,\n",
    "    \"Coefficient\": model.coef_\n",
    "})\n",
    "\n",
    "print(coefficients.sort_values(by=\"Coefficient\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cc1d1d7-1299-42cf-8ace-630608d953ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 3. The model explains around 73% of the variance in the training data and 65% in the testing data, therefore it fits well but not perfectly of course. The features that contribute the most are grade, \\nlatitude, waterfront, and sqft_living, which have the largest coefficients and have a big influence on housing price. The training MSE is lower than the testing MSE, \\nwhich means the data is a bit overfitted but is still a reasonable generalization. '"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Problem 2 continued #######\n",
    "\n",
    "\"\"\" 3. The model explains around 73% of the variance in the training data and 65% in the testing data, therefore it fits well but not perfectly of course. The features that contribute the most are grade, \n",
    "latitude, waterfront, and sqft_living, which have the largest coefficients and have a big influence on housing price. The training MSE is lower than the testing MSE, \n",
    "which means the data is a bit overfitted but is still a reasonable generalization. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffa6cf89-b60e-4e95-bc57-801e87a32799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 34334.80751744762\n",
      "Train R2: 0.7017921632750106\n",
      "Test MSE: 59948.01817993596\n",
      "Test R2: 0.6404419393707315\n"
     ]
    }
   ],
   "source": [
    "##### Problem 3:  Implementing closed-form solution for linear regression #####\n",
    "\n",
    "X_train_bias = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n",
    "X_test_bias = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]\n",
    "\n",
    "theta = np.linalg.inv(X_train_bias.T @ X_train_bias) @ X_train_bias.T @ y_train.values\n",
    "\n",
    "y_train_pred_cf = X_train_bias @ theta\n",
    "y_test_pred_cf = X_test_bias @ theta\n",
    "\n",
    "print(\"Train MSE:\", mean_squared_error(y_train, y_train_pred_cf))\n",
    "print(\"Train R2:\", r2_score(y_train, y_train_pred_cf))\n",
    "print(\"Test MSE:\", mean_squared_error(y_test, y_test_pred_cf))\n",
    "print(\"Test R2:\", r2_score(y_test, y_test_pred_cf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9415200-e6a1-466e-8e08-e2d1b093e8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3 Discussion\n",
    "\n",
    "\"\"\" Compared to the sk learn model I chose to use from Problem 2, the results are very similar, with only small differences in MSE and R squared,\n",
    "which means this function is accurate \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93228490-5dc3-4c7d-8b82-2d7982d4cfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Degree</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>57947.526161</td>\n",
       "      <td>88575.978543</td>\n",
       "      <td>0.496709</td>\n",
       "      <td>0.468736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>54822.665116</td>\n",
       "      <td>71791.679479</td>\n",
       "      <td>0.523849</td>\n",
       "      <td>0.569406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>53785.194716</td>\n",
       "      <td>99833.483763</td>\n",
       "      <td>0.532860</td>\n",
       "      <td>0.401216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52626.111955</td>\n",
       "      <td>570616.914820</td>\n",
       "      <td>0.542927</td>\n",
       "      <td>-2.422464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Degree     Train MSE       Test MSE  Train R2   Test R2\n",
       "0       1  57947.526161   88575.978543  0.496709  0.468736\n",
       "1       2  54822.665116   71791.679479  0.523849  0.569406\n",
       "2       3  53785.194716   99833.483763  0.532860  0.401216\n",
       "3       5  52626.111955  570616.914820  0.542927 -2.422464"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Problem 4: Polynomial Regression ####\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    X_poly = np.ones((X.shape[0], 1))\n",
    "    for d in range(1, degree + 1):\n",
    "        X_poly = np.c_[X_poly, X**d]\n",
    "    return X_poly\n",
    "\n",
    "feature = \"sqft_living\"\n",
    "\n",
    "X_train_sqft = train[[feature]].values\n",
    "X_test_sqft = test[[feature]].values\n",
    "\n",
    "scaler_sqft = StandardScaler()\n",
    "X_train_sqft = scaler_sqft.fit_transform(X_train_sqft)\n",
    "X_test_sqft = scaler_sqft.transform(X_test_sqft)\n",
    "\n",
    "results = []\n",
    "\n",
    "for p in [1,2,3,5]:\n",
    "    X_train_poly = polynomial_features(X_train_sqft, p)\n",
    "    X_test_poly = polynomial_features(X_test_sqft, p)\n",
    "\n",
    "    theta_poly = np.linalg.inv(X_train_poly.T @ X_train_poly) @ X_train_poly.T @ y_train.values\n",
    "\n",
    "    y_train_pred = X_train_poly @ theta_poly\n",
    "    y_test_pred = X_test_poly @ theta_poly\n",
    "\n",
    "    results.append([\n",
    "        p,\n",
    "        mean_squared_error(y_train, y_train_pred),\n",
    "        mean_squared_error(y_test, y_test_pred),\n",
    "        r2_score(y_train, y_train_pred),\n",
    "        r2_score(y_test, y_test_pred)\n",
    "    ])\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=[\"Degree\", \"Train MSE\", \"Test MSE\", \"Train R2\", \"Test R2\"])\n",
    "results_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4c3a1aa-1c8c-42ae-8858-c7552557e22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' As the polynomial degree increases, the training MSE decreases and training R2 increases slightly, which is expected since more complex models fit the training data better. \\nBut, the testing performance improves only up to degree 2 and then worsens significantly for higher degrees. Like for example degree 5, the testing R2 becomes negative.\\nThis shows that while adding polynomial terms increases model flexibility, higher-degree polynomials might not generalize well. '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Problem 4 Cont ####\n",
    "\n",
    "\"\"\" As the polynomial degree increases, the training MSE decreases and training R2 increases slightly, which is expected since more complex models fit the training data better. \n",
    "But, the testing performance improves only up to degree 2 and then worsens significantly for higher degrees. Like for example degree 5, the testing R2 becomes negative.\n",
    "This shows that while adding polynomial terms increases model flexibility, higher-degree polynomials might not generalize well. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "efd1088c-44d3-4e2e-af5a-01cd0381e623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.01 Iterations: 10\n",
      "Theta: [ 4.97609866e+01 -5.71355940e-01  7.88230818e+00  1.28885855e+01\n",
      "  1.94697369e+01  3.65596272e+00  6.19596850e+00  9.65192771e+00\n",
      "  1.30928688e+01  2.52788366e+00  1.79913003e+01  1.58321121e+01\n",
      "  1.05991846e+01 -9.86859516e-01  4.55126922e+00  1.13477059e+01\n",
      "  1.14764514e-02  1.78147002e+01  4.02621859e+00]\n",
      "Alpha: 0.01 Iterations: 50\n",
      "Theta: [ 2.05560702e+02 -3.15750389e-02  1.25163535e+01  2.59212818e+01\n",
      "  4.79069491e+01  5.45299794e+00  1.16687762e+01  3.27540471e+01\n",
      "  3.92741651e+01  1.03813881e+01  4.62406985e+01  3.72421944e+01\n",
      "  2.90836406e+01 -1.65007790e+01  1.58020462e+01  4.09624652e+01\n",
      " -7.61827526e+00  4.41320794e+01  5.68597809e+00]\n",
      "Alpha: 0.01 Iterations: 100\n",
      "Theta: [329.92617388   2.14260858   6.03789753  23.83369358  54.60665253\n",
      "   3.66935537  11.13059106  46.01543057  49.51375953  14.50279947\n",
      "  56.5436346   42.4780124   33.10264224 -31.88671648  21.22889322\n",
      "  59.96091374 -13.09620479  51.20633612   2.71040894]\n",
      "Alpha: 0.1 Iterations: 10\n",
      "Theta: [338.95740148   2.22154276   5.70590219  23.70581216  55.21020435\n",
      "   3.41133273  11.00793979  47.16226034  50.74031493  15.00809845\n",
      "  57.13356136  42.75049428  33.81373292 -33.18283679  21.90575365\n",
      "  61.58346078 -14.0043119   51.79391794   2.48104355]\n",
      "Alpha: 0.1 Iterations: 50\n",
      "Theta: [517.73273293   7.95522105 -11.4634934   16.57652923  57.92985935\n",
      "   4.65756979   8.47673026  62.53379121  48.63925846  14.86413122\n",
      "  79.92340162  49.39266311  27.53079176 -59.14044011  20.08603221\n",
      "  80.22709982  -6.26356746  53.45127867  -6.43093339]\n",
      "Alpha: 0.1 Iterations: 100\n",
      "Theta: [520.40101105   8.35212216 -12.70675998  17.56078578  57.57438066\n",
      "   8.02115648   8.09311821  63.88449575  47.4078302   13.33962046\n",
      "  88.04653874  48.92442824  27.65122229 -65.28700644  18.20987457\n",
      "  78.84704475  -3.01801266  48.96543826 -10.04831151]\n",
      "Alpha: 0.5 Iterations: 10\n",
      "Theta: [  519.90661639   184.95534589 -2649.03270802 -3902.13640294\n",
      " -4289.04506715 -1661.45730325 -2423.94847542  -468.15889126\n",
      " -1209.83174272   557.99971262 -3952.66813392 -4203.26066291\n",
      " -1081.10185041 -2593.65487682    49.45868257  -342.86683508\n",
      " -1998.60210829 -4045.76782261 -1806.76190128]\n",
      "Alpha: 0.5 Iterations: 50\n",
      "Theta: [ 5.20417490e+02  2.94524885e+10 -4.39070974e+11 -6.52409929e+11\n",
      " -7.23757491e+11 -2.77399101e+11 -4.04940194e+11 -8.84103329e+10\n",
      " -2.09460672e+11  9.04373610e+10 -6.71516195e+11 -7.08068336e+11\n",
      " -1.84557364e+11 -4.21863362e+11  4.92325470e+09 -7.04561995e+10\n",
      " -3.31786100e+11 -6.82496194e+11 -2.99695221e+11]\n",
      "Alpha: 0.5 Iterations: 100\n",
      "Theta: [ 5.03881102e+07  5.57015893e+20 -8.30386576e+21 -1.23386077e+22\n",
      " -1.36879580e+22 -5.24627005e+21 -7.65837237e+21 -1.67204753e+21\n",
      " -3.96139442e+21  1.71038340e+21 -1.26999521e+22 -1.33912392e+22\n",
      " -3.49041423e+21 -7.97842931e+21  9.31103363e+19 -1.33249260e+21\n",
      " -6.27485624e+21 -1.29076097e+22 -5.66794216e+21]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Train MSE</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Train R2</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>2.947967e+05</td>\n",
       "      <td>3.524490e+05</td>\n",
       "      <td>-1.560395e+00</td>\n",
       "      <td>-1.113929e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>50</td>\n",
       "      <td>1.382985e+05</td>\n",
       "      <td>1.704509e+05</td>\n",
       "      <td>-2.011631e-01</td>\n",
       "      <td>-2.233580e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>7.009438e+04</td>\n",
       "      <td>9.475122e+04</td>\n",
       "      <td>3.912098e-01</td>\n",
       "      <td>4.316983e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10</td>\n",
       "      <td>10</td>\n",
       "      <td>6.647361e+04</td>\n",
       "      <td>9.087301e+04</td>\n",
       "      <td>4.226573e-01</td>\n",
       "      <td>4.549591e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10</td>\n",
       "      <td>50</td>\n",
       "      <td>3.151072e+04</td>\n",
       "      <td>5.892963e+04</td>\n",
       "      <td>7.263202e-01</td>\n",
       "      <td>6.465501e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>100</td>\n",
       "      <td>3.142750e+04</td>\n",
       "      <td>5.889174e+04</td>\n",
       "      <td>7.270429e-01</td>\n",
       "      <td>6.467773e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.50</td>\n",
       "      <td>10</td>\n",
       "      <td>6.163610e+08</td>\n",
       "      <td>6.888098e+08</td>\n",
       "      <td>-5.352276e+03</td>\n",
       "      <td>-4.130365e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.50</td>\n",
       "      <td>50</td>\n",
       "      <td>1.708464e+25</td>\n",
       "      <td>1.904481e+25</td>\n",
       "      <td>-1.483851e+20</td>\n",
       "      <td>-1.142275e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.50</td>\n",
       "      <td>100</td>\n",
       "      <td>6.110787e+45</td>\n",
       "      <td>6.811893e+45</td>\n",
       "      <td>-5.307397e+40</td>\n",
       "      <td>-4.085658e+40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alpha  Iterations     Train MSE      Test MSE      Train R2       Test R2\n",
       "0   0.01          10  2.947967e+05  3.524490e+05 -1.560395e+00 -1.113929e+00\n",
       "1   0.01          50  1.382985e+05  1.704509e+05 -2.011631e-01 -2.233580e-02\n",
       "2   0.01         100  7.009438e+04  9.475122e+04  3.912098e-01  4.316983e-01\n",
       "3   0.10          10  6.647361e+04  9.087301e+04  4.226573e-01  4.549591e-01\n",
       "4   0.10          50  3.151072e+04  5.892963e+04  7.263202e-01  6.465501e-01\n",
       "5   0.10         100  3.142750e+04  5.889174e+04  7.270429e-01  6.467773e-01\n",
       "6   0.50          10  6.163610e+08  6.888098e+08 -5.352276e+03 -4.130365e+03\n",
       "7   0.50          50  1.708464e+25  1.904481e+25 -1.483851e+20 -1.142275e+20\n",
       "8   0.50         100  6.110787e+45  6.811893e+45 -5.307397e+40 -4.085658e+40"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Problem 5:  Gradient descent #####\n",
    "\n",
    "def gradient_descent(X, y, alpha, iterations):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(X.shape[1])\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta\n",
    "        gradient = (1/m) * X.T @ (predictions - y)\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "    return theta\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "X_train_bias = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n",
    "X_test_bias = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]\n",
    "\n",
    "\n",
    "for alpha in [0.01, 0.1, 0.5]:\n",
    "    for it in [10, 50, 100]:\n",
    "        theta_gd = gradient_descent(X_train_bias, y_train.values, alpha, it)\n",
    "\n",
    "        y_train_pred = X_train_bias @ theta_gd\n",
    "        y_test_pred = X_test_bias @ theta_gd\n",
    "\n",
    "        results.append([\n",
    "            alpha,\n",
    "            it,\n",
    "            mean_squared_error(y_train, y_train_pred),\n",
    "            mean_squared_error(y_test, y_test_pred),\n",
    "            r2_score(y_train, y_train_pred),\n",
    "            r2_score(y_test, y_test_pred)\n",
    "        ])\n",
    "\n",
    "        print(\"Alpha:\", alpha, \"Iterations:\", it)\n",
    "        print(\"Theta:\", theta_gd)\n",
    "\n",
    "results_df = pd.DataFrame(results, \n",
    "    columns=[\"Alpha\", \"Iterations\", \"Train MSE\", \"Test MSE\", \"Train R2\", \"Test R2\"])\n",
    "\n",
    "results_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e003b76-7933-4a45-b5df-58fca378a33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' When α = 0.01, the convergence is slow and performance improves gradually but does not reach the optimal solution by 100 iterations. When α = 0.1, it converges and reaches almost an optimal MSE and R² values after 50 iterations. With α = 0.5, the algorithm diverges, causing larger error and worse R² values. This shows that an appropriate learning rate is necessary for stable convergence to the optimal solution.\\n\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" When α = 0.01, the convergence is slow and performance improves gradually but does not reach the optimal solution by 100 iterations. When α = 0.1, it converges and reaches almost an optimal MSE and R² values after 50 iterations. With α = 0.5, the algorithm diverges, causing larger error and worse R² values. This shows that an appropriate learning rate is necessary for stable convergence to the optimal solution.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6287c915-5997-4cd8-9996-ed221c43dc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS slope: 1.9656920054163671\n",
      "OLS MSE: 1.8653740489434376\n",
      "OLS R2: 0.7367593726211263\n",
      "\n",
      "Lambda: 1\n",
      "Slope: 1.964238266463562\n",
      "MSE: 1.865376904433118\n",
      "R2: 0.7367589696558897\n",
      "\n",
      "Lambda: 10\n",
      "Slope: 1.9512507370045533\n",
      "MSE: 1.865655834299756\n",
      "R2: 0.7367196072164208\n",
      "\n",
      "Lambda: 100\n",
      "Slope: 1.8302356836661557\n",
      "MSE: 1.8901657483079775\n",
      "R2: 0.7332607807444963\n",
      "\n",
      "Lambda: 1000\n",
      "Slope: 1.1296410740692593\n",
      "MSE: 2.809811521006989\n",
      "R2: 0.6034808418047767\n",
      "\n",
      "Lambda: 10000\n",
      "Slope: 0.23398221709987252\n",
      "MSE: 5.917267005381879\n",
      "R2: 0.164958320425167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##### Problem 6: Ridge regularization #####\n",
    "\n",
    "def ridge_gradient_descent(X, y, alpha, iterations, lam):\n",
    "    m = len(y)\n",
    "    theta = np.zeros(X.shape[1])\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        predictions = X @ theta\n",
    "        gradient = (1/m) * (X.T @ (predictions - y))\n",
    "        \n",
    "        # Regularization term (exclude bias)\n",
    "        reg = 2 * lam * theta\n",
    "        reg[0] = 0\n",
    "        \n",
    "        theta = theta - alpha * (gradient + reg)\n",
    "\n",
    "    return theta\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N = 1000\n",
    "X = np.random.uniform(-2, 2, N)\n",
    "eps = np.random.normal(0, np.sqrt(2), N)\n",
    "Y = 1 + 2*X + eps\n",
    "\n",
    "X_design = np.column_stack((np.ones(N), X))\n",
    "\n",
    "def ridge_closed_form(X, y, lam):\n",
    "    I = np.eye(X.shape[1])\n",
    "    I[0,0] = 0  # don't regularize bias\n",
    "    return np.linalg.inv(X.T @ X + lam * I) @ X.T @ y\n",
    "\n",
    "theta_ols = ridge_closed_form(X_design, Y, 0)\n",
    "y_pred_ols = X_design @ theta_ols\n",
    "\n",
    "print(\"OLS slope:\", theta_ols[1])\n",
    "print(\"OLS MSE:\", mean_squared_error(Y, y_pred_ols))\n",
    "print(\"OLS R2:\", r2_score(Y, y_pred_ols))\n",
    "print()\n",
    "\n",
    "for lam in [1,10,100,1000,10000]:\n",
    "    theta_ridge = ridge_closed_form(X_design, Y, lam)\n",
    "    y_pred = X_design @ theta_ridge\n",
    "    \n",
    "    print(\"Lambda:\", lam)\n",
    "    print(\"Slope:\", theta_ridge[1])\n",
    "    print(\"MSE:\", mean_squared_error(Y, y_pred))\n",
    "    print(\"R2:\", r2_score(Y, y_pred))\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4902b227-16e2-4b91-9a69-ef409cabbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observations: As λ increases, the slope goes toward 0, the coefficients become smaller, training MSE increases, R² decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61b1fc-4f20-4a41-acd8-8aff564e45a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
